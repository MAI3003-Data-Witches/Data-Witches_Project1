{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MAI3003-Data-Witches/Data-Witches_Project1/blob/main/MAI3003_Data_Witches_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **MAI3003 - Data Witches**"
   ],
   "metadata": {
    "id": "eIxmxpw9QQk6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Name**  | **Student ID**\n",
    "-------------------|------------------\n",
    "Claessen, VVHJAE | i6339543\n",
    "Ovsiannikova, AM | i6365923\n",
    "Pubben, J | i6276134\n",
    "Roca Cugat, M | i6351071\n",
    "Záboj, J| i6337952"
   ],
   "metadata": {
    "id": "Ozgt3F8Vx69M"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logbook"
   ],
   "metadata": {
    "id": "ZL_Vnj8xMmJH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "▶▶**Please make a copy of the notebook to work in. Let's keep this one as our final notebook (and updated with code ofc).**◀◀"
   ],
   "metadata": {
    "id": "8JBlUWtSEbj1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Changes**\n",
    "Also see Git Commit History.\n",
    "\n",
    "| **Version** | **Changes**      | **Date** |\n",
    "|-------------|------------------|----------|\n",
    "| v0.0        | Dataset loaded   | 02-11-25 |\n",
    "| v0.1        | Data exploration | 05-11-25 |\n",
    "| v0.2        | Data cleaning    | 05-11-25 |\n"
   ],
   "metadata": {
    "id": "sq1Vxq3hMmJI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Methods\n",
    "\n",
    "Let's ensure we all use the same names for all components.  \n",
    "\n",
    "| **Variable**         | **Name**       |\n",
    "|----------------------|----------------|\n",
    "| Dataframe (raw)      | hcv            |\n",
    "| Encoded dataframe    | hcv_encoded    |\n",
    "| Imputed dataframe    | hcv_imputed    |\n",
    "| Dataframe unskewed   | hcv_unskewed   |\n",
    "| Dataframe normalized | hcv_normalized |\n",
    "\n",
    "| **Function**              | **Description**                        | **Arguments**                                    |\n",
    "|---------------------------|----------------------------------------|--------------------------------------------------|\n",
    "| corr_plot()               | Correlation plot                       | df                                               |\n",
    "| distplots()               | Distribution plots                     | df                                               |\n",
    "| drop_high_missing_cols()  | Drop columns with missings             | df, threshold                                    |\n",
    "| _reconstruct_dataframe()  | Reconstruction function for imputation | encoded_df, original_df, num_cols, cat_cols, enc |\n",
    "| knn_impute()              | Distribution plots                     | df, min_thresh, max_thresh, n_neighbors          |\n",
    "| impute_simple_central()   | Distribution plots                     | df, max_thresh                                   |\n",
    "| normality_check_and_fix() | Distribution plots                     | df                                               |\n",
    "| skewness_check()          | Distribution plots                     | df                                               |\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Z1gtQqZJmSU2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preamble"
   ],
   "metadata": {
    "id": "05y6UtfnFis-"
   }
  },
  {
   "metadata": {
    "id": "qnx7f8c2XmgK"
   },
   "cell_type": "markdown",
   "source": [
    "## Libraries and other imports"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -r requirements.txt"
   ],
   "metadata": {
    "id": "5_VL_kMQXy0P",
    "outputId": "7c00e437-9aaf-4798-a7c3-3eb41852f905",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: colorama\n",
      "Successfully installed colorama-0.4.6\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import levene\n",
    "\n",
    "import numpy as np\n",
    "from numpy.ma.core import indices\n",
    "\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import warnings"
   ],
   "metadata": {
    "id": "I1Z0IQDyFo_2",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:21.761515Z",
     "start_time": "2025-11-05T13:56:21.758873Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:21.809261Z",
     "start_time": "2025-11-05T13:56:21.806907Z"
    },
    "id": "dw127owMXmgL"
   },
   "cell_type": "code",
   "source": [
    "missing  = 0\n",
    "misVariables = []\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "print(Style.RESET_ALL)\n",
    "\n",
    "# Change False to True during development, they are used to ignore warnings when in \"prod\".\n",
    "if True: warnings.filterwarnings('ignore')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "wJ2AWZ1JXmgL"
   },
   "cell_type": "markdown",
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "hcv = pd.read_csv('https://archive.ics.uci.edu/static/public/571/data.csv')\n",
    "\n",
    "hcv"
   ],
   "metadata": {
    "id": "hFjjGsasGDL4",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:23.319648Z",
     "start_time": "2025-11-05T13:56:21.865559Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "34ZwaG7gXmgM"
   },
   "cell_type": "markdown",
   "source": [
    "## Function declarations"
   ]
  },
  {
   "metadata": {
    "id": "0j5TZrW-XmgM"
   },
   "cell_type": "markdown",
   "source": [
    "### Graphs"
   ]
  },
  {
   "metadata": {
    "id": "LLcRLtBQXmgM"
   },
   "cell_type": "markdown",
   "source": [
    "#### Correlation plot"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:23.498942Z",
     "start_time": "2025-11-05T13:56:23.496675Z"
    },
    "id": "6Vf6S6BEXmgM"
   },
   "cell_type": "code",
   "source": [
    "def corr_plot(input_df):\n",
    "    sns.set_theme(style=\"white\")\n",
    "    corr = input_df.select_dtypes('number').corr()\n",
    "\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    f, ax = plt.subplots(figsize=(20, 5))\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.5, vmin=-0.5, center=0,annot = False,\n",
    "                 linewidths=.5, cbar_kws={\"shrink\": .8})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8EJAPDSLXmgM"
   },
   "cell_type": "markdown",
   "source": [
    "#### Distribution plots"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:23.603088Z",
     "start_time": "2025-11-05T13:56:23.600024Z"
    },
    "id": "Gg-VhTWOXmgM"
   },
   "cell_type": "code",
   "source": [
    "def distplots(df):\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "    num_features = len(numeric_df.columns)\n",
    "    cols = int(np.ceil(np.sqrt(num_features)))\n",
    "    rows = int(np.ceil(num_features / cols))\n",
    "\n",
    "    # A figure with subplots looks much nicer\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, column in enumerate(numeric_df.columns):\n",
    "        # Technically not needed but might as well\n",
    "        numeric_df_nona = numeric_df[column].dropna()\n",
    "\n",
    "        axes[i].hist(numeric_df_nona, bins=30, alpha=0.7, edgecolor='black')\n",
    "\n",
    "        if len(numeric_df_nona) > 1:\n",
    "            density = stats.gaussian_kde(numeric_df_nona)\n",
    "            xs = np.linspace(numeric_df_nona.min(), numeric_df_nona.max(), 200)\n",
    "            axes[i].plot(xs, density(xs) * len(numeric_df_nona) * (numeric_df_nona.max() - numeric_df_nona.min()) / 30,\n",
    "                         'r-', linewidth=2)\n",
    "\n",
    "        axes[i].set_xlabel(column)\n",
    "        axes[i].set_ylabel('Number of Patients')\n",
    "        axes[i].set_title(f'Distribution of {column}')\n",
    "        axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Remove any empty subplots if they exist\n",
    "    for j in range(num_features, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "zsRfJ_ymXmgN"
   },
   "cell_type": "markdown",
   "source": [
    "### Imputation"
   ]
  },
  {
   "metadata": {
    "id": "kg7fXRDJXmgN"
   },
   "cell_type": "markdown",
   "source": [
    "#### Drop high missing columns"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:23.706971Z",
     "start_time": "2025-11-05T13:56:23.704311Z"
    },
    "id": "syMTZrxQXmgN"
   },
   "cell_type": "code",
   "source": [
    "def drop_high_missing_cols(df, threshold):\n",
    "    \"\"\"\n",
    "    1. Drops columns with a high percentage of missing values.\n",
    "    \"\"\"\n",
    "    print(f\"--- Running Step 1: Dropping columns > {threshold:.0%} missing ---\")\n",
    "    # Calculate missing percentage on the dataframe\n",
    "    missing_pct = df.isnull().sum() / len(df)\n",
    "\n",
    "    # Identify columns to drop based on the threshold\n",
    "    cols_to_drop = missing_pct[missing_pct >= threshold].index.tolist()\n",
    "\n",
    "    if cols_to_drop:\n",
    "        print(f\"   -> Dropping columns: {', '.join(cols_to_drop)}\")\n",
    "        # Drop columns from the dataframe\n",
    "        df_dropped = df.drop(columns=cols_to_drop)\n",
    "    else:\n",
    "        print(\"   -> No columns exceeded the missing value threshold.\")\n",
    "        df_dropped = df.copy()\n",
    "\n",
    "    return df_dropped"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "rVisRr0ZXmgN"
   },
   "cell_type": "markdown",
   "source": [
    "#### Dataframe reconstruction\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:23.752741Z",
     "start_time": "2025-11-05T13:56:23.750026Z"
    },
    "id": "1R6RQ6qzXmgN"
   },
   "cell_type": "code",
   "source": [
    "def _reconstruct_dataframe(encoded_df, original_df, num_cols, cat_cols, enc):\n",
    "    \"\"\"\n",
    "    Internal helper function to revert a one-hot encoded DataFrame\n",
    "    back to its original shape.\n",
    "    \"\"\"\n",
    "    # Isolate the imputed numerical data\n",
    "    # Ensure we only select num_cols that are still in the encoded_df\n",
    "    present_num_cols = [col for col in num_cols if col in encoded_df.columns]\n",
    "    imputed_numerical = encoded_df[present_num_cols]\n",
    "\n",
    "    # Isolate the encoded columns to be inverse-transformed\n",
    "    encoded_cols_names = enc.get_feature_names_out(cat_cols)\n",
    "\n",
    "    # Ensure all expected encoded columns are present, fill with 0 if not\n",
    "    for col in encoded_cols_names:\n",
    "        if col not in encoded_df.columns:\n",
    "            encoded_df[col] = 0\n",
    "\n",
    "    imputed_encoded = encoded_df[encoded_cols_names]\n",
    "\n",
    "    # Perform the inverse transform\n",
    "    imputed_categorical_array = enc.inverse_transform(imputed_encoded)\n",
    "\n",
    "    # Convert the result back to a DataFrame\n",
    "    imputed_categorical = pd.DataFrame(imputed_categorical_array,\n",
    "                                       columns=cat_cols,\n",
    "                                       index=encoded_df.index)\n",
    "\n",
    "    # Combine numerical and reverted categorical data\n",
    "    reconstructed_df = pd.concat([imputed_numerical, imputed_categorical], axis=1)\n",
    "\n",
    "    # Enforce the original column order\n",
    "    # Use .columns.intersection() to avoid errors if columns were dropped\n",
    "    original_cols_present = original_df.columns.intersection(reconstructed_df.columns)\n",
    "    final_df = reconstructed_df.reindex(columns=original_cols_present)\n",
    "\n",
    "    return final_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "y-GvC17pXmgN"
   },
   "cell_type": "markdown",
   "source": [
    "#### KNN imputation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:23.802177Z",
     "start_time": "2025-11-05T13:56:23.797360Z"
    },
    "id": "aZQwlh86XmgN"
   },
   "cell_type": "code",
   "source": [
    "def knn_impute(df, min_thresh, max_thresh, n_neighbors):\n",
    "    \"\"\"\n",
    "    2. Uses KNN Imputation for columns with moderate missing values (5%-50%).\n",
    "\n",
    "    What the function does:\n",
    "    - One-hot encodes categorical data.\n",
    "    - Trains KNN models (Regressor or Classifier) on the data\n",
    "      to predict its own missing values.\n",
    "    - Reconstructs the dataframe back to its original format.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Step 2: KNN Imputation ({min_thresh:.0%} - {max_thresh:.0%} missing) ---\")\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "    # --- 1. One-Hot Encoding ---\n",
    "\n",
    "    # Identify categorical/numerical columns\n",
    "    categorical_cols = [col for col in df_imputed.columns\n",
    "                        if df_imputed[col].dtype == 'object']\n",
    "    numerical_cols = [col for col in df_imputed.columns\n",
    "                      if df_imputed[col].dtype != 'object']\n",
    "\n",
    "    # Initialize encoder\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    # Handle case with no categorical columns\n",
    "    if not categorical_cols:\n",
    "        print(\"   -> No categorical columns found. Skipping encoding.\")\n",
    "        df_encoded = df_imputed.copy()\n",
    "        encoder = None  # Flag that no encoder was used\n",
    "    else:\n",
    "        print(f\"   -> Fitting OneHotEncoder on {len(categorical_cols)} columns...\")\n",
    "        # Fit and transform the data\n",
    "        encoded_cols_df = pd.DataFrame(encoder.fit_transform(df_imputed[categorical_cols]),\n",
    "                                       columns=encoder.get_feature_names_out(categorical_cols),\n",
    "                                       index=df_imputed.index)\n",
    "\n",
    "        # Create the new, fully encoded dataframe\n",
    "        df_encoded = df_imputed.drop(columns=categorical_cols).join(encoded_cols_df)\n",
    "        print(f\"   -> Data encoded. New shape: {df_encoded.shape}\")\n",
    "\n",
    "    # --- 2. KNN Imputation on Data ---\n",
    "\n",
    "    # Calculate means *once* to be used for filling features (not targets)\n",
    "    feature_fill_values = df_encoded.mean()\n",
    "\n",
    "    for col in df_encoded.columns:\n",
    "        missing_pct = df_encoded[col].isnull().mean()\n",
    "\n",
    "        # Apply KNN to columns with moderate missingness\n",
    "        if min_thresh <= missing_pct <= max_thresh:\n",
    "            print(f\"   -> KNN Imputing '{col}' (Missing: {missing_pct:.2%})\")\n",
    "\n",
    "            other_cols = [c for c in df_encoded.columns if c != col]\n",
    "\n",
    "            # Split data for the imputer model\n",
    "            train_rows = df_encoded[col].notnull()\n",
    "            predict_rows = df_encoded[col].isnull()\n",
    "\n",
    "            # If no rows to predict, skip\n",
    "            if not predict_rows.any():\n",
    "                print(f\"      -> Skipping '{col}', no rows to predict.\")\n",
    "                continue\n",
    "\n",
    "            # Fill NaNs in *features* with the mean for model stability\n",
    "            X_train = df_encoded.loc[train_rows, other_cols].fillna(feature_fill_values)\n",
    "            y_train = df_encoded.loc[train_rows, col]\n",
    "            X_predict = df_encoded.loc[predict_rows, other_cols].fillna(feature_fill_values)\n",
    "\n",
    "            # Select and train the appropriate KNN model\n",
    "            is_binary = y_train.nunique() <= 2\n",
    "            model = KNeighborsClassifier(n_neighbors=n_neighbors) if is_binary else KNeighborsRegressor(\n",
    "                n_neighbors=n_neighbors)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            predicted_values = model.predict(X_predict)\n",
    "\n",
    "            # Assign the predicted values back to the encoded dataframe\n",
    "            df_encoded.loc[predict_rows, col] = predicted_values\n",
    "            print(f\"      -> Successfully imputed {len(predicted_values)} values.\")\n",
    "\n",
    "    # --- 3. Reconstruct DataFrame ---\n",
    "\n",
    "    # If no encoding was done, just return the dataframe\n",
    "    if encoder is None:\n",
    "        print(\"   -> Skipping reconstruction (no categorical columns).\")\n",
    "        return df_encoded\n",
    "\n",
    "    print(\"   -> Reconstructing dataframe...\")\n",
    "    # Reconstruct the dataframe\n",
    "    df_reconstructed = _reconstruct_dataframe(\n",
    "        df_encoded,\n",
    "        df_imputed,  # Pass the original copy for column order\n",
    "        numerical_cols,\n",
    "        categorical_cols,\n",
    "        encoder\n",
    "    )\n",
    "    df_reconstructed.sort_index(inplace=True)\n",
    "\n",
    "    return df_reconstructed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "fJlhHvH8XmgO"
   },
   "cell_type": "markdown",
   "source": [
    "#### Normal imputation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:23.847959Z",
     "start_time": "2025-11-05T13:56:23.845152Z"
    },
    "id": "CDw-oUDAXmgO"
   },
   "cell_type": "code",
   "source": [
    "def impute_simple_central(df, max_thresh):\n",
    "    \"\"\"\n",
    "    3. Uses simple imputation (median/mode) for columns with < 5% missing values.\n",
    "\n",
    "    This function learns the imputation value (median for numeric, mode for\n",
    "    categorical) from the dataframe and applies it to fill its own NaNs.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Step 3: Simple Imputation (< {max_thresh:.0%} missing) ---\")\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "    for col in df_imputed.columns:\n",
    "        # Calculate the percentage of missing values\n",
    "        missing_pct = df_imputed[col].isnull().mean()\n",
    "\n",
    "        # Check if the column fits the < 5% criteria\n",
    "        if 0 < missing_pct < max_thresh:\n",
    "            print(f\"   -> Found '{col}' with {missing_pct:.2%} missing values. Imputing...\")\n",
    "\n",
    "            # Distinguish between numerical and categorical data\n",
    "            if pd.api.types.is_numeric_dtype(df_imputed[col]):\n",
    "                # For numerical columns, use the median\n",
    "                fill_value = df_imputed[col].median()\n",
    "                df_imputed[col].fillna(fill_value, inplace=True)\n",
    "                print(f\"      -> Filled with median: {fill_value}\")\n",
    "            else:\n",
    "                # For categorical columns, use the mode\n",
    "                fill_value = df_imputed[col].mode()[0]\n",
    "                df_imputed[col].fillna(fill_value, inplace=True)\n",
    "                print(f\"      -> Filled with mode: '{fill_value}'\")\n",
    "\n",
    "    print(\"   -> Simple imputation complete.\")\n",
    "    return df_imputed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "urjKvOUcXmgO"
   },
   "cell_type": "markdown",
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "metadata": {
    "id": "vwxvPzhPXmgO"
   },
   "cell_type": "markdown",
   "source": [
    "#### Data encoding"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:23.893883Z",
     "start_time": "2025-11-05T13:56:23.891505Z"
    },
    "id": "OnCbOV8nXmgO"
   },
   "cell_type": "code",
   "source": [
    "def encode_categorical(df):\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    categorical_cols = hcv.select_dtypes(include=['object']).columns\n",
    "\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(hcv[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"Encoded '{col}': {hcv[col].unique()[:5]} -> {df_encoded[col].unique()[:5]}\")\n",
    "\n",
    "    return df_encoded"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "MnmancjnXmgO"
   },
   "cell_type": "markdown",
   "source": [
    "#### Normality check and correction"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:23.951004Z",
     "start_time": "2025-11-05T13:56:23.946975Z"
    },
    "id": "BBOfKdHRXmgO"
   },
   "cell_type": "code",
   "source": [
    "def normality_check_and_fix(df):\n",
    "    df_normalized = df.copy()\n",
    "    print(\"Shapiro-Wilk Normality Test\")\n",
    "\n",
    "    for column in df.columns:\n",
    "        data_nona = df[column].dropna()\n",
    "        stat, p_value = stats.shapiro(data_nona)\n",
    "\n",
    "        if p_value > 0.05:\n",
    "            print(Fore.GREEN + f\"{column}: Normal (p={p_value:.4f})\")\n",
    "        else:\n",
    "            print(Fore.RED + f\"{column}: Not Normal (p={p_value:.4f})\")\n",
    "            df_normalized[column] = normalize(df[column])\n",
    "    print(Style.RESET_ALL)\n",
    "    return df_normalized"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:23.995783Z",
     "start_time": "2025-11-05T13:56:23.992951Z"
    },
    "id": "ICIXBMDsXmgP"
   },
   "cell_type": "code",
   "source": [
    "def normalize(df): # Only to be run on non-normal columns\n",
    "    data_nona = df.dropna()\n",
    "    skewness = stats.skew(data_nona)\n",
    "    df_normalized = df.copy()\n",
    "    if skewness > 0.5:  # Right-skewed\n",
    "        # Log transformation (with handling of zero-values)\n",
    "        if (data_nona > 0).all():\n",
    "            df_normalized = np.log(df)\n",
    "            print(f\"      -> Applied log transformation\")\n",
    "        else:\n",
    "            df_normalized = np.log1p(df)\n",
    "            print(f\"      -> Applied log1p transformation\")\n",
    "    else:\n",
    "        # Box-Cox transformation (requires positive values)\n",
    "        if (data_nona > 0).all():\n",
    "            transformed, _ = stats.boxcox(data_nona)\n",
    "            df_normalized.loc[data_nona.index] = transformed\n",
    "            print(f\"      -> Applied Box-Cox transformation\")\n",
    "        else:\n",
    "            # Square root transformation for zero-values\n",
    "            if (data_nona >= 0).all():\n",
    "                df_normalized = np.sqrt(df)\n",
    "                print(f\"      -> Applied square root transformation\")\n",
    "            else:\n",
    "                print(f\"      -> Skipped (contains negative values)\")\n",
    "\n",
    "    return df_normalized"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "QR90OBucXmgP"
   },
   "cell_type": "markdown",
   "source": [
    "#### Skewness check"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:24.042805Z",
     "start_time": "2025-11-05T13:56:24.040209Z"
    },
    "id": "ZSI0MYnAXmgP"
   },
   "cell_type": "code",
   "source": [
    "def skewness_check(df):\n",
    "    for column in df:\n",
    "        skewness = stats.skew(df[column])\n",
    "        if skewness > 1:\n",
    "            print(Fore.RED + f\"{column} is strongly right skewed (skew: {skewness:.3f})\")\n",
    "        elif skewness > 0.5:\n",
    "            print(Fore.RED + f\"{column} is moderately right skewed (skew: {skewness:.3f})\")\n",
    "        elif stats.skew(df[column]) < -1:\n",
    "            print(Fore.MAGENTA + f\"{column} is strongly left skewed (skew: {skewness:.3f})\")\n",
    "        elif stats.skew(df[column]) < -0.5:\n",
    "            print(Fore.MAGENTA + f\"{column} is moderately left skewed (skew: {skewness:.3f})\")\n",
    "        else:\n",
    "            print(Fore.GREEN + f\"{column} is symmetric (skew: {skewness:.3f})\")\n",
    "    print(Style.RESET_ALL)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "PEIfTGLCXmgP"
   },
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:24.102083Z",
     "start_time": "2025-11-05T13:56:24.093610Z"
    },
    "id": "YNV0-GxbXmgP"
   },
   "cell_type": "code",
   "source": [
    "hcv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:24.231720Z",
     "start_time": "2025-11-05T13:56:24.216923Z"
    },
    "id": "zrbT3WkHXmgP"
   },
   "cell_type": "code",
   "source": [
    "hcv.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:26.030274Z",
     "start_time": "2025-11-05T13:56:24.692593Z"
    },
    "id": "SVaMOoXJXmgP"
   },
   "cell_type": "code",
   "source": [
    "distplots(hcv)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:26.205553Z",
     "start_time": "2025-11-05T13:56:26.090654Z"
    },
    "id": "pwomQfTPXmgQ"
   },
   "cell_type": "code",
   "source": [
    "corr_plot(hcv.select_dtypes(include=['number']))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:37.355532Z",
     "start_time": "2025-11-05T13:56:26.220859Z"
    },
    "id": "HM1iAfcgXmgQ"
   },
   "cell_type": "code",
   "source": [
    "sns.pairplot(hcv, hue='Category')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "MFOX7WOGXmgQ"
   },
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:37.447758Z",
     "start_time": "2025-11-05T13:56:37.439080Z"
    },
    "id": "3RqL5TUnXmgQ"
   },
   "cell_type": "code",
   "source": [
    "hcv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "uoO9tqQEXmgQ"
   },
   "cell_type": "markdown",
   "source": [
    "## Encoding"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:37.570721Z",
     "start_time": "2025-11-05T13:56:37.561059Z"
    },
    "id": "nDFBLhBHXmgQ"
   },
   "cell_type": "code",
   "source": [
    "hcv_encoded = encode_categorical(hcv)\n",
    "hcv_encoded"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8sLsCUiHXmgQ"
   },
   "cell_type": "markdown",
   "source": [
    "## Outlier check"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:38.085191Z",
     "start_time": "2025-11-05T13:56:38.064510Z"
    },
    "id": "y9Q4IV2MXmgR"
   },
   "cell_type": "code",
   "source": [
    "numerical_cols = hcv_encoded.select_dtypes(include=np.number).columns\n",
    "outlier_indices = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    Q1 = hcv_encoded[col].quantile(0.25)\n",
    "    Q3 = hcv_encoded[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    indices = hcv_encoded[(hcv_encoded[col] < lower_bound) | (hcv_encoded[col] > upper_bound)].index\n",
    "    if len(indices) > 0:\n",
    "            outlier_indices[col] = indices.tolist()\n",
    "            #print(f'{len(indices)} outliers found in {col} column')\n",
    "    else:\n",
    "        pass\n",
    "        #print(f\"No outlier data found for column {col}\")\n",
    "\n",
    "outlier_values = []\n",
    "outliers_count = 0\n",
    "for col, rows in outlier_indices.items():\n",
    "    outlier_values.append([col, len(rows), (len(rows)*10/ len(hcv_encoded[col]))])\n",
    "    outliers_count = outliers_count + 1 if len(rows) > 0 else outliers_count\n",
    "\n",
    "if outliers_count > 0:\n",
    "    data_outliers = pd.DataFrame(outlier_values)\n",
    "    data_outliers.columns = ['Variable', 'Outliers', 'Percentage Outliers']\n",
    "    s = data_outliers.sort_values(by=['Percentage Outliers'], ascending=False).style.bar(subset=['Percentage Outliers'], color='#d65f5f')\n",
    "    display(s)\n",
    "else:\n",
    "    print('No outlier values found in the dataset.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "VT8laridXmgR"
   },
   "cell_type": "markdown",
   "source": [
    "## Missing check"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "CheckNull = hcv_encoded.isnull().sum()\n",
    "for var in range(0, len(CheckNull)):\n",
    "    misVariables.append([hcv_encoded.columns[var], CheckNull[var], round(CheckNull[var]*10/len(hcv_encoded),3)])\n",
    "    missing = missing + 1\n",
    "\n",
    "if missing == 0:\n",
    "    print('Dataset is complete with no blanks.')\n",
    "else:\n",
    "    data_misVariables = pd.DataFrame.from_records(misVariables)\n",
    "    data_misVariables.columns = ['Variable', 'Missing', 'Percentage missing']\n",
    "    s = data_misVariables.sort_values(by=['Percentage missing'], ascending=False).style.bar(subset=['Percentage missing'], color='#d65f5f')\n",
    "    display(s)"
   ],
   "metadata": {
    "id": "lnOpesMVNjwt",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:38.142807Z",
     "start_time": "2025-11-05T13:56:38.134225Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "jqkvBUdoXmgR"
   },
   "cell_type": "markdown",
   "source": [
    "## Imputation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:38.213245Z",
     "start_time": "2025-11-05T13:56:38.196116Z"
    },
    "id": "2NYHw6laXmgS"
   },
   "cell_type": "code",
   "source": [
    "hcv_dropped = drop_high_missing_cols(hcv_encoded, threshold=0.50)\n",
    "hcv_knn_predicted= knn_impute(hcv_dropped, min_thresh=0.02, max_thresh=0.50, n_neighbors=5)\n",
    "hcv_imputed = impute_simple_central(hcv_knn_predicted, max_thresh=0.05)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:38.282267Z",
     "start_time": "2025-11-05T13:56:38.267685Z"
    },
    "id": "B7pTLdjEXmgS"
   },
   "cell_type": "code",
   "source": [
    "hcv_imputed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "kCm8CbvjXmgS"
   },
   "cell_type": "markdown",
   "source": [
    "## Normalisation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T14:06:56.180715Z",
     "start_time": "2025-11-05T14:06:55.086361Z"
    },
    "id": "3VoZ0NppXmgS"
   },
   "cell_type": "code",
   "source": [
    "distplots(hcv)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T14:06:18.784885Z",
     "start_time": "2025-11-05T14:06:18.727272Z"
    },
    "id": "lEXZqS86XmgS"
   },
   "cell_type": "code",
   "source": [
    "hcv_normalized = normality_check_and_fix(hcv_imputed.drop(columns=['Category','Sex'])).join(hcv_imputed[['Category', 'Sex']])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "OasU5hQSXmgS"
   },
   "cell_type": "markdown",
   "source": [
    "## Skewness"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T14:06:39.717974Z",
     "start_time": "2025-11-05T14:06:39.707766Z"
    },
    "id": "VNNR4jRqXmgT"
   },
   "cell_type": "code",
   "source": [
    "skewness_check(hcv_imputed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:39.733577Z",
     "start_time": "2025-11-05T13:56:38.752015Z"
    },
    "id": "SUzAth_HXmgT"
   },
   "cell_type": "code",
   "source": [
    "distplots(hcv_imputed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Logistic Regression - All features**"
   ],
   "metadata": {
    "id": "GoO3qM2iNdkY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Logistic Regression Model**"
   ],
   "metadata": {
    "id": "xpNugOycPqSZ"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "-Y4-KQtGNizK",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:39.788883Z",
     "start_time": "2025-11-05T13:56:39.787530Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Forest Plot**"
   ],
   "metadata": {
    "id": "a6SvtVKNPuPQ"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "giIWHiWBPyBc",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:39.833626Z",
     "start_time": "2025-11-05T13:56:39.832270Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Logistic Regression - Selected features**"
   ],
   "metadata": {
    "id": "ytMy8kfENkLS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Feature selection**"
   ],
   "metadata": {
    "id": "MjmKYj6-PKqt"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "OT2nb7ktNo5r",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:39.878145Z",
     "start_time": "2025-11-05T13:56:39.876858Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Training/test set**\n"
   ],
   "metadata": {
    "id": "hjtpbMuOPMlF"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "9LI36ENnPQLT",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:39.922880Z",
     "start_time": "2025-11-05T13:56:39.921498Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **K-fold cross-validation**"
   ],
   "metadata": {
    "id": "19UDL9bOPP8N"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "5HJ5M-thPXBT",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:39.968585Z",
     "start_time": "2025-11-05T13:56:39.967222Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Probabilistic model selection (AIC/BIC)**"
   ],
   "metadata": {
    "id": "MjIOFcTXPTNC"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "BLCXZ0I2PTAB",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:40.013163Z",
     "start_time": "2025-11-05T13:56:40.011882Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Logistic Regression - Ranking**"
   ],
   "metadata": {
    "id": "NhajNdbNNpZJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main considerations\n",
    "**1. What were your main considerations when selecting the best model?**\n"
   ],
   "metadata": {
    "id": "j_vSAquyN877"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "UJwAaqldO1GN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model explanation - Positive diagnosis\n",
    "**2. Explain how your best model makes the decision for a positive diagnosis.**\n",
    "\n"
   ],
   "metadata": {
    "id": "w4WXsFEnOLVU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "B5p9BQUYOWeE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model explanation - Over/underfitting, anchoring biases\n",
    "**3. Explain how your proposed model addresses overfitting, underfitting, or anchoring biases *(choose one)* in training.**"
   ],
   "metadata": {
    "id": "tORIrUOWONVJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "OpiaYTK1OYKJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Random Forest**"
   ],
   "metadata": {
    "id": "0KXiAKOdNvFe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Random Forest model**"
   ],
   "metadata": {
    "id": "753340CFO8El"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8qEnObb5NzBD",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:40.057377Z",
     "start_time": "2025-11-05T13:56:40.055962Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Feature importance measures**\n",
    "\n"
   ],
   "metadata": {
    "id": "AnN6V1A9O_Cq"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "cGW7DxPaPDyZ",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:40.102051Z",
     "start_time": "2025-11-05T13:56:40.100765Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **SHAP values**"
   ],
   "metadata": {
    "id": "b0e3pl2mPESF"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "eC-4ZEQDPFbB",
    "ExecuteTime": {
     "end_time": "2025-11-05T13:56:40.146152Z",
     "start_time": "2025-11-05T13:56:40.144879Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
